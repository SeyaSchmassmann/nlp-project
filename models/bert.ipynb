{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "IS_PAPERSPACE = os.getcwd().startswith('/notebooks')\n",
    "dir_env = os.path.join(os.getcwd(), '.env') if IS_PAPERSPACE else os.path.join(os.getcwd(), '..', '.env')\n",
    "_ = load_dotenv(dotenv_path=dir_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_traindata = pd.read_csv(\"data/train.csv\")\n",
    "\n",
    "train_texts, validation_texts, train_labels, validation_labels = train_test_split(\n",
    "    df_traindata['text'].tolist(), \n",
    "    df_traindata['sentiment'].tolist(), \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "df_testdata = pd.read_csv(\"data/test.csv\")\n",
    "test_texts = df_testdata['text'].tolist()\n",
    "test_labels = df_testdata['sentiment'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
    "validation_encodings = tokenizer(validation_texts, truncation=True, padding=True, max_length=128)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            key: torch.tensor(val[idx]) \n",
    "            for key, val in self.encodings.items()\n",
    "        } | {\n",
    "            'labels': torch.tensor(self.labels[idx])\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = TweetDataset(train_encodings, train_labels)\n",
    "validation_dataset = TweetDataset(validation_encodings, validation_labels)\n",
    "test_dataset = TweetDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "    }\n",
    "\n",
    "try:\n",
    "    os.environ[\"WANDB_API_KEY\"] = os.getenv('API_KEY_WANDB')\n",
    "    wandb.login(key=os.getenv('API_KEY_WANDB'))\n",
    "\n",
    "    os.environ[\"WANDB_PROJECT\"] = \"nlp-lantsch-schmassmann-wigger\"\n",
    "    os.environ[\"WANDB_ENTITY\"] = \"nlp-lantsch-schmassmann-wigger\"\n",
    "    os.environ[\"WANDB_WATCH\"] = \"all\"\n",
    "    \n",
    "    wandb.init(\n",
    "        project=os.getenv('WANDB_PROJECT'),\n",
    "        entity=os.getenv('WANDB_ENTITY'),\n",
    "        config={\n",
    "            \"learning_rate\": 2e-5,\n",
    "            \"epochs\": 3,\n",
    "            \"batch_size\": 16\n",
    "        }\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=64,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./logs\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        save_total_limit=1,\n",
    "        report_to=[\"wandb\"] \n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=validation_dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.evaluate()\n",
    "    test_results = trainer.predict(test_dataset)\n",
    "\n",
    "    metrics = compute_metrics((test_results.predictions, test_results.label_ids))\n",
    "\n",
    "    wandb.log({\n",
    "        \"test_precision\": metrics[\"precision\"],\n",
    "        \"test_recall\": metrics[\"recall\"],\n",
    "        \"test_f1\": metrics[\"f1\"]\n",
    "    })\n",
    "\n",
    "    wandb.finish()\n",
    "except Exception as e:\n",
    "    wandb.finish()\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
