{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ae232bb-43df-4245-9739-d4c50463146b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 13:20:44.357073: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-28 13:20:44.365950: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748438444.375260   18663 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748438444.378431   18663 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748438444.386488   18663 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748438444.386498   18663 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748438444.386498   18663 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748438444.386499   18663 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-28 13:20:44.389672: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "afd4ed0b-832d-4d15-8840-970e0cb479e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseComponent():\n",
    "    def forward(self, input_data):\n",
    "        \"\"\"Forward pass through the component.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        \"\"\"Backward pass through the component.\"\"\"\n",
    "        pass\n",
    "\n",
    "class RNNCell(BaseComponent):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.W_x = np.random.randn(hidden_size, input_size) * 0.01\n",
    "        self.W_h = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.W_y = np.random.randn(output_size, hidden_size) * 0.01\n",
    "        self.b_h = np.zeros((hidden_size, 1))\n",
    "        self.b_y = np.zeros((output_size, 1))\n",
    "\n",
    "    def forward(self, x_t, h_prev):\n",
    "        \"\"\"Compute the hidden state and output for a single time step.\"\"\"\n",
    "        self.x_t = x_t\n",
    "        self.h_prev = h_prev\n",
    "\n",
    "        # Hidden state update\n",
    "        self.h_t = np.tanh(np.dot(self.W_x, x_t) + np.dot(self.W_h, h_prev) + self.b_h)\n",
    "\n",
    "        # Output calculation\n",
    "        self.y_t = self.softmax(np.dot(self.W_y, self.h_t) + self.b_y)\n",
    "\n",
    "        return self.h_t, self.y_t\n",
    "\n",
    "    def backward(self, dy, dh_next):\n",
    "        \"\"\"Compute gradients for weights and biases.\"\"\"\n",
    "        dW_y = np.dot(dy, self.h_t.T)\n",
    "        db_y = dy\n",
    "\n",
    "        dh = np.dot(self.W_y.T, dy) + dh_next\n",
    "        dh_raw = (1 - self.h_t ** 2) * dh\n",
    "\n",
    "        dW_x = np.dot(dh_raw, self.x_t.T)\n",
    "        dW_h = np.dot(dh_raw, self.h_prev.T)\n",
    "        db_h = dh_raw\n",
    "\n",
    "        dh_prev = np.dot(self.W_h.T, dh_raw)\n",
    "\n",
    "        # Store gradients\n",
    "        self.grads = {\n",
    "            \"W_x\": dW_x,\n",
    "            \"W_h\": dW_h,\n",
    "            \"W_y\": dW_y,\n",
    "            \"b_h\": db_h,\n",
    "            \"b_y\": db_y,\n",
    "        }\n",
    "\n",
    "        return dh_prev\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        exp_x = np.exp(x - np.max(x))\n",
    "        return exp_x / exp_x.sum(axis=0, keepdims=True)\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.cell = RNNCell(input_size, hidden_size, output_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"Process the entire sequence.\"\"\"\n",
    "        self.h_states = []\n",
    "        self.outputs = []\n",
    "\n",
    "        h_t = np.zeros((self.cell.hidden_size, 1))  # Initial hidden state\n",
    "\n",
    "        for x_t in X:\n",
    "            h_t, y_t = self.cell.forward(x_t, h_t)\n",
    "            self.h_states.append(h_t)\n",
    "            self.outputs.append(y_t)\n",
    "\n",
    "        return self.outputs\n",
    "\n",
    "    def backward(self, dY):\n",
    "        \"\"\"Backpropagate through the entire sequence.\"\"\"\n",
    "        dh_next = np.zeros_like(self.h_states[0])\n",
    "        for t in reversed(range(len(self.h_states))):\n",
    "            dh_next = self.cell.backward(dY[t], dh_next)\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, rnn, learning_rate=0.01):\n",
    "        self.rnn = rnn\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def compute_loss(self, y_pred, y_true):\n",
    "        \"\"\"Compute cross-entropy loss.\"\"\"\n",
    "        return -np.sum(y_true * np.log(y_pred + 1e-8))\n",
    "\n",
    "    def update_parameters(self):\n",
    "        \"\"\"Update weights and biases using gradients.\"\"\"\n",
    "        for param, grad in self.rnn.cell.grads.items():\n",
    "            setattr(self.rnn.cell, param, getattr(self.rnn.cell, param) - self.learning_rate * grad)\n",
    "\n",
    "    def train(self, X, Y, epochs):\n",
    "        \"\"\"Train the RNN on a dataset of tokenized sentences.\"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "    \n",
    "            data = list(zip(X, Y))\n",
    "            np.random.shuffle(data)\n",
    "            X, Y = zip(*data)\n",
    "    \n",
    "            for x_seq, y_true in zip(X, Y):\n",
    "                # Forward pass\n",
    "                y_pred = self.rnn.forward(x_seq)[-1]  # Use the final output for classification\n",
    "    \n",
    "                # Compute loss\n",
    "                loss = self.compute_loss(y_pred, y_true)  # Directly compute loss for scalars\n",
    "                total_loss += loss\n",
    "    \n",
    "                # Backward pass\n",
    "                dY = y_pred - y_true  # Gradient of loss w.r.t. prediction\n",
    "                self.rnn.backward([dY])  # Wrap dY in a list to match expected input\n",
    "    \n",
    "                # Update parameters\n",
    "                self.update_parameters()\n",
    "    \n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(X)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1779a728-6a90-4534-b7c7-255b1dd0bfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_traindata = pd.read_csv(\"../data/train.csv\")\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(df_traindata['sentiment'].tolist())\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(df_traindata['text'].tolist())\n",
    "sequences = tokenizer.texts_to_sequences(df_traindata['text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2077bbb-1977-4694-98c4-8a801cb2ac17",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m trainer = Trainer(RNN(\u001b[32m100\u001b[39m, \u001b[32m128\u001b[39m, \u001b[32m1\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 121\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, X, Y, epochs)\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m    120\u001b[39m dY = y_pred - y_true  \u001b[38;5;66;03m# Gradient of loss w.r.t. prediction\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdY\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Wrap dY in a list to match expected input\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Update parameters\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;28mself\u001b[39m.update_parameters()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 86\u001b[39m, in \u001b[36mRNN.backward\u001b[39m\u001b[34m(self, dY)\u001b[39m\n\u001b[32m     84\u001b[39m dh_next = np.zeros_like(\u001b[38;5;28mself\u001b[39m.h_states[\u001b[32m0\u001b[39m])\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.h_states))):\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     dh_next = \u001b[38;5;28mself\u001b[39m.cell.backward(\u001b[43mdY\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m, dh_next)\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(RNN(100, 128, 1))\n",
    "\n",
    "trainer.train(sequences, labels, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ae29e3-1d94-4d5f-b1c8-4252f0cbb991",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
